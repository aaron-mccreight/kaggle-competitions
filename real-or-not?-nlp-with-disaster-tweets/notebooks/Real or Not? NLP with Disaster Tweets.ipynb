{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real or Not? NLP with Disaster Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle competition. Link: https://www.kaggle.com/c/nlp-getting-started/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Set-up](#Set-up)\n",
    "2. [Data Analysis](#Data-Analysis)\n",
    "2. [Feature Engineering Functions](#Feature-Engineering-Functions)\n",
    "4. [Data Preparation](#Data-Preparation)\n",
    "5. [Scikit Learn Machine Learning](#Scikit-Learn-Machine-Learning)\n",
    "6. [Keras Machine Learning](#Keras-Machine-Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit Learn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Keras\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0  1   NaN     NaN       \n",
       "1  4   NaN     NaN       \n",
       "2  5   NaN     NaN       \n",
       "3  6   NaN     NaN       \n",
       "4  7   NaN     NaN       \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all                                                                   \n",
       "1  Forest fire near La Ronge Sask. Canada                                                                                                  \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3  13,000 people receive #wildfires evacuation orders in California                                                                        \n",
       "4  Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school                                                 \n",
       "\n",
       "   target  \n",
       "0  1       \n",
       "1  1       \n",
       "2  1       \n",
       "3  1       \n",
       "4  1       "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0   \n",
       "keyword     61  \n",
       "location    2533\n",
       "text        0   \n",
       "target      0   \n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4342\n",
      "1    3271\n",
      "Name: target, dtype: int64\n",
      "0    0.57034\n",
      "1    0.42966\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_test['target'].value_counts())\n",
    "print(df_test['target'].value_counts(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at hastags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0  1   NaN     NaN       \n",
       "1  4   NaN     NaN       \n",
       "2  5   NaN     NaN       \n",
       "3  6   NaN     NaN       \n",
       "4  7   NaN     NaN       \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all                                                                   \n",
       "1  Forest fire near La Ronge Sask. Canada                                                                                                  \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3  13,000 people receive #wildfires evacuation orders in California                                                                        \n",
       "4  Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school                                                 \n",
       "\n",
       "   target  hashtag  \n",
       "0  1       True     \n",
       "1  1       False    \n",
       "2  1       False    \n",
       "3  1       True     \n",
       "4  1       True     "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['hashtag'] = df_test['text'].apply(lambda tweet: '#' in tweet)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.503123\n",
      "1    0.496877\n",
      "Name: target, dtype: float64\n",
      "0    0.590567\n",
      "1    0.409433\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_test[df_test['hashtag']]['target'].value_counts(1))\n",
    "print(df_test[~df_test['hashtag']]['target'].value_counts(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at if there's any punctuation (right now just . , ' \" ; :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0  1   NaN     NaN       \n",
       "1  4   NaN     NaN       \n",
       "2  5   NaN     NaN       \n",
       "3  6   NaN     NaN       \n",
       "4  7   NaN     NaN       \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all                                                                   \n",
       "1  Forest fire near La Ronge Sask. Canada                                                                                                  \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3  13,000 people receive #wildfires evacuation orders in California                                                                        \n",
       "4  Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school                                                 \n",
       "\n",
       "   target  hashtag  punctuation  \n",
       "0  1       True     False        \n",
       "1  1       False    True         \n",
       "2  1       False    True         \n",
       "3  1       True     True         \n",
       "4  1       True     False        "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['punctuation'] = df_test['text'].apply(lambda tweet: '.' in tweet or ',' in tweet or \",\" in tweet or '\"' in tweet or ';' in tweet or ':' in tweet)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.519102\n",
      "1    0.480898\n",
      "Name: target, dtype: float64\n",
      "0    0.756231\n",
      "1    0.243769\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_test[df_test['punctuation']]['target'].value_counts(1))\n",
    "print(df_test[~df_test['punctuation']]['target'].value_counts(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some numerical stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>punctuation</th>\n",
       "      <th>tweet length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0  1   NaN     NaN       \n",
       "1  4   NaN     NaN       \n",
       "2  5   NaN     NaN       \n",
       "3  6   NaN     NaN       \n",
       "4  7   NaN     NaN       \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all                                                                   \n",
       "1  Forest fire near La Ronge Sask. Canada                                                                                                  \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3  13,000 people receive #wildfires evacuation orders in California                                                                        \n",
       "4  Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school                                                 \n",
       "\n",
       "   target  hashtag  punctuation  tweet length  \n",
       "0  1       True     False        69            \n",
       "1  1       False    True         38            \n",
       "2  1       False    True         133           \n",
       "3  1       True     True         65            \n",
       "4  1       True     False        88            "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['tweet length'] = df_test['text'].apply(lambda tweet: len(tweet))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>punctuation</th>\n",
       "      <th>tweet length</th>\n",
       "      <th>number of words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>133</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>65</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>88</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0  1   NaN     NaN       \n",
       "1  4   NaN     NaN       \n",
       "2  5   NaN     NaN       \n",
       "3  6   NaN     NaN       \n",
       "4  7   NaN     NaN       \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all                                                                   \n",
       "1  Forest fire near La Ronge Sask. Canada                                                                                                  \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3  13,000 people receive #wildfires evacuation orders in California                                                                        \n",
       "4  Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school                                                 \n",
       "\n",
       "   target  hashtag  punctuation  tweet length  number of words  \n",
       "0  1       True     False        69            13               \n",
       "1  1       False    True         38            7                \n",
       "2  1       False    True         133           22               \n",
       "3  1       True     True         65            9                \n",
       "4  1       True     False        88            17               "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['number of words'] = df_test['text'].apply(lambda tweet: len(tweet.split(' ')))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>punctuation</th>\n",
       "      <th>tweet length</th>\n",
       "      <th>number of words</th>\n",
       "      <th>number of sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>133</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>65</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>88</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0  1   NaN     NaN       \n",
       "1  4   NaN     NaN       \n",
       "2  5   NaN     NaN       \n",
       "3  6   NaN     NaN       \n",
       "4  7   NaN     NaN       \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all                                                                   \n",
       "1  Forest fire near La Ronge Sask. Canada                                                                                                  \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3  13,000 people receive #wildfires evacuation orders in California                                                                        \n",
       "4  Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school                                                 \n",
       "\n",
       "   target  hashtag  punctuation  tweet length  number of words  \\\n",
       "0  1       True     False        69            13                \n",
       "1  1       False    True         38            7                 \n",
       "2  1       False    True         133           22                \n",
       "3  1       True     True         65            9                 \n",
       "4  1       True     False        88            17                \n",
       "\n",
       "   number of sentences  \n",
       "0  1                    \n",
       "1  2                    \n",
       "2  2                    \n",
       "3  1                    \n",
       "4  1                    "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['number of sentences'] = df_test['text'].apply(lambda tweet: len(tweet.split('.')))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Forest fire near La Ronge Sask. Canada'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['text'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                     0.060781\n",
       "target                 1.000000\n",
       "hashtag                0.074486\n",
       "punctuation            0.197150\n",
       "tweet length           0.181817\n",
       "number of words        0.040862\n",
       "number of sentences    0.156265\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.corr()['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>punctuation</th>\n",
       "      <th>tweet length</th>\n",
       "      <th>number of words</th>\n",
       "      <th>number of sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.060781</td>\n",
       "      <td>-0.008026</td>\n",
       "      <td>0.012285</td>\n",
       "      <td>0.017393</td>\n",
       "      <td>0.005267</td>\n",
       "      <td>0.018247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0.060781</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.074486</td>\n",
       "      <td>0.197150</td>\n",
       "      <td>0.181817</td>\n",
       "      <td>0.040862</td>\n",
       "      <td>0.156265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hashtag</th>\n",
       "      <td>-0.008026</td>\n",
       "      <td>0.074486</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.126030</td>\n",
       "      <td>0.208775</td>\n",
       "      <td>0.055172</td>\n",
       "      <td>0.070796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>punctuation</th>\n",
       "      <td>0.012285</td>\n",
       "      <td>0.197150</td>\n",
       "      <td>0.126030</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.403420</td>\n",
       "      <td>0.168680</td>\n",
       "      <td>0.517039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet length</th>\n",
       "      <td>0.017393</td>\n",
       "      <td>0.181817</td>\n",
       "      <td>0.208775</td>\n",
       "      <td>0.403420</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.831328</td>\n",
       "      <td>0.401347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of words</th>\n",
       "      <td>0.005267</td>\n",
       "      <td>0.040862</td>\n",
       "      <td>0.055172</td>\n",
       "      <td>0.168680</td>\n",
       "      <td>0.831328</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.223232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of sentences</th>\n",
       "      <td>0.018247</td>\n",
       "      <td>0.156265</td>\n",
       "      <td>0.070796</td>\n",
       "      <td>0.517039</td>\n",
       "      <td>0.401347</td>\n",
       "      <td>0.223232</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id    target   hashtag  punctuation  tweet length  \\\n",
       "id                   1.000000  0.060781 -0.008026  0.012285     0.017393       \n",
       "target               0.060781  1.000000  0.074486  0.197150     0.181817       \n",
       "hashtag             -0.008026  0.074486  1.000000  0.126030     0.208775       \n",
       "punctuation          0.012285  0.197150  0.126030  1.000000     0.403420       \n",
       "tweet length         0.017393  0.181817  0.208775  0.403420     1.000000       \n",
       "number of words      0.005267  0.040862  0.055172  0.168680     0.831328       \n",
       "number of sentences  0.018247  0.156265  0.070796  0.517039     0.401347       \n",
       "\n",
       "                     number of words  number of sentences  \n",
       "id                   0.005267         0.018247             \n",
       "target               0.040862         0.156265             \n",
       "hashtag              0.055172         0.070796             \n",
       "punctuation          0.168680         0.517039             \n",
       "tweet length         0.831328         0.401347             \n",
       "number of words      1.000000         0.223232             \n",
       "number of sentences  0.223232         1.000000             "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_test[df_test['target']==0]['text'].sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test[df_test['target']==1]['text'].sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_substring_in_tweet_column(substring, df):\n",
    "    \"\"\"Function will add new column to df.\n",
    "    New column will be of boolean type: True if substring is in that row's tweet, False otherwise.\n",
    "    Note: if substring is all lowercase, will assume case does not matter and so will search if substring is in\n",
    "    lowercase version of tweet. If case matters, then substring must contain at least one capital. This is an assumption.\"\"\"\n",
    "    \n",
    "    new_column_name = \"'\" + substring + \"' in tweet?\" # can tell if case matters if substring has capitals\n",
    "    match_case = not substring == substring.lower()\n",
    "    \n",
    "    if match_case:\n",
    "        df[new_column_name] = df['text'].apply(lambda tweet: substring in tweet)\n",
    "    \n",
    "    else:\n",
    "        df[new_column_name] = df['text'].apply(lambda tweet: substring in tweet.lower())\n",
    "        \n",
    "# add_substring_in_tweet_column('.', df_test)\n",
    "# add_substring_in_tweet_column('our', df_test)\n",
    "# add_substring_in_tweet_column('fire', df_test)\n",
    "# display(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment_analysis_column(df, method):\n",
    "    \"\"\"Takes in a dataframe, df, and adds sentiment analysis columns to it using a specified method.\n",
    "    If no correct method is specified, nothing will be added to the dataframe.\n",
    "    Columns add are: 'neg', 'neu', 'pos' and 'compound'.\n",
    "    Possible methods: nltk_vader\n",
    "    \"\"\"\n",
    "    \n",
    "    if method == 'nltk_vader':\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        df['sentiment'] = df['text'].apply(sid.polarity_scores)\n",
    "        df['neg'] = df['sentiment'].apply(lambda sent: sent['neg'])\n",
    "        df['neu'] = df['sentiment'].apply(lambda sent: sent['neu'])\n",
    "        df['pos'] = df['sentiment'].apply(lambda sent: sent['pos'])\n",
    "        df['compound'] = df['sentiment'].apply(lambda sent: sent['compound'])\n",
    "        df.drop('sentiment', axis=1, inplace=True)\n",
    "        \n",
    "    else:\n",
    "        print('No valid method entered - no new columns will be added to the dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spacy_and_other_numeric_columns(df):\n",
    "    \"\"\"Adds columns to a dataframe, df, that use the library spacy.\n",
    "    Since spacy provides some interesting numeric results which will be added as columns,\n",
    "    this function will also add the other numeric columns as well, since they may depend on these results.\"\"\"\n",
    "    \n",
    "    # First, create a doc column for each tweet (i.e. tokenize each tweet)\n",
    "    #df['doc'] = df['text'].apply(lambda tweet: nlp(tweet))\n",
    "    generator = nlp.pipe(df['text'])\n",
    "    spacy_dict = {'number of tokens':[], 'number of sentences':[], 'number of words':[],\n",
    "                  'number of nouns':[], 'number of verbs':[], 'number of adjectives':[],\n",
    "                  'number of adverbs':[], 'number of pronouns':[], 'number of punctuation':[]}\n",
    "    \n",
    "    for doc in generator:\n",
    "        # Number of tokens (not sure if it's that useful)\n",
    "        spacy_dict['number of tokens'].append(len(doc))\n",
    "        \n",
    "        # Number of sentences\n",
    "        num_sentences = len([sentence for sentence in doc.sents])\n",
    "        spacy_dict['number of sentences'].append(num_sentences)\n",
    "        \n",
    "        # Number of words (including stop words)\n",
    "        num_words = len([token.text for token in doc if token.is_punct != True])\n",
    "        spacy_dict['number of words'].append(num_words)\n",
    "\n",
    "        # Number of nouns (use first line if don't want stop words or punctuations)\n",
    "        #num_nouns = len([token.text for token in doc if token.is_stop != True and token.is_punct != True and token.pos_ == \"NOUN\"])\n",
    "        num_nouns = len([token.text for token in doc if token.pos_ == \"NOUN\"])\n",
    "        spacy_dict['number of nouns'].append(num_nouns)\n",
    "        \n",
    "        # Number of verbs\n",
    "        num_verbs = len([token.text for token in doc if token.pos_ == \"VERB\"])\n",
    "        spacy_dict['number of verbs'].append(num_verbs)\n",
    "        \n",
    "        # Number of adjectives\n",
    "        num_adj = len([token.text for token in doc if token.pos_ == \"ADJ\"])\n",
    "        spacy_dict['number of adjectives'].append(num_adj)\n",
    "        \n",
    "        # Number of adverbs\n",
    "        num_adverbs = len([token.text for token in doc if token.pos_ == \"ADV\"])\n",
    "        spacy_dict['number of adverbs'].append(num_adverbs)\n",
    "        \n",
    "        # Number of pronouns\n",
    "        num_pronouns = len([token.text for token in doc if token.pos_ == \"PRON\"])\n",
    "        spacy_dict['number of pronouns'].append(num_pronouns)\n",
    "        \n",
    "        # Number of punctuation symbols used \n",
    "        # (Note: not sure if hastags are included - first tweet hashtag is considered a noun)\n",
    "        # Could maybe use is_punct instead??\n",
    "        num_punct = len([token.text for token in doc if token.pos_ == \"PUNCT\"])\n",
    "        spacy_dict['number of punctuation'].append(num_punct)\n",
    "        \n",
    "    \n",
    "    spacy_df = pd.DataFrame(data=spacy_dict)\n",
    "    df = pd.concat([df, spacy_df], axis=1)\n",
    "    \n",
    "    \n",
    "    df['tweet length'] = df['text'].apply(lambda tweet: len(tweet))\n",
    "    df['word length (including whitespace)'] = df['tweet length'] / df['number of words']\n",
    "    df['word per sentence'] = df['number of words'] / df['number of sentences']\n",
    "    df['total hastags'] = df['text'].apply(lambda tweet: len(re.findall('#', tweet)))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vectorization_columns(df, method='tfidf', min_df=0.0, max_df=1.0, stop_words='english'):\n",
    "    \"\"\"Adds vectorization columns to dataframe(s) based on tweets.\n",
    "    \n",
    "    Notes:\n",
    "    -Possible methods are 'count' (CountVectorizer) and 'tfidf' (TfidfVectorizer)\n",
    "    -Can also specify min_df, max_df and stop_words\n",
    "    \"\"\"\n",
    "    \n",
    "    try:     \n",
    "        # Set up vectorization\n",
    "        if method == 'count':\n",
    "            vectorizer = CountVectorizer(min_df=min_df, max_df=max_df,stop_words=stop_words)\n",
    "\n",
    "        if method == 'tfidf':\n",
    "            vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df,stop_words=stop_words)        \n",
    "\n",
    "        # Add vectors to df and return\n",
    "        vectors = vectorizer.fit_transform(df[\"text\"])\n",
    "        vectors = pd.DataFrame(vectors.todense())\n",
    "        df = pd.concat([df, vectors], axis=1)\n",
    "        return df\n",
    "        \n",
    "    except:\n",
    "        print('No valid method entered!')\n",
    "\n",
    "        \n",
    "# add_vectorization_columns(train_df,'tfidf',50,0.9,'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['id', 'keyword', 'location', 'text']\n",
    "substring_list = ['#', '.', ',', ':', \"'\", 'http']\n",
    "spacy_cols = True\n",
    "spacy_and_num_cols = True\n",
    "sentiment_analysis_method = 'nltk_vader'\n",
    "encode_keyword = False # Haven't found this to be too useful\n",
    "\n",
    "vectorize_method = 'tfidf' # very slow\n",
    "min_df = 2 #0.0\n",
    "max_df = 1.0\n",
    "stop_words = 'english'\n",
    "\n",
    "ml_model = 'ridge'\n",
    "nearest_neighbors = 1\n",
    "\n",
    "nn_hidden_layers = 2\n",
    "nn_num_neurons = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>'#' in tweet?</th>\n",
       "      <th>'.' in tweet?</th>\n",
       "      <th>',' in tweet?</th>\n",
       "      <th>':' in tweet?</th>\n",
       "      <th>''' in tweet?</th>\n",
       "      <th>'http' in tweet?</th>\n",
       "      <th>number of tokens</th>\n",
       "      <th>number of sentences</th>\n",
       "      <th>number of words</th>\n",
       "      <th>number of nouns</th>\n",
       "      <th>number of verbs</th>\n",
       "      <th>number of adjectives</th>\n",
       "      <th>number of adverbs</th>\n",
       "      <th>number of pronouns</th>\n",
       "      <th>number of punctuation</th>\n",
       "      <th>tweet length</th>\n",
       "      <th>word length (including whitespace)</th>\n",
       "      <th>word per sentence</th>\n",
       "      <th>total hastags</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>...</th>\n",
       "      <th>6281</th>\n",
       "      <th>6282</th>\n",
       "      <th>6283</th>\n",
       "      <th>6284</th>\n",
       "      <th>6285</th>\n",
       "      <th>6286</th>\n",
       "      <th>6287</th>\n",
       "      <th>6288</th>\n",
       "      <th>6289</th>\n",
       "      <th>6290</th>\n",
       "      <th>6291</th>\n",
       "      <th>6292</th>\n",
       "      <th>6293</th>\n",
       "      <th>6294</th>\n",
       "      <th>6295</th>\n",
       "      <th>6296</th>\n",
       "      <th>6297</th>\n",
       "      <th>6298</th>\n",
       "      <th>6299</th>\n",
       "      <th>6300</th>\n",
       "      <th>6301</th>\n",
       "      <th>6302</th>\n",
       "      <th>6303</th>\n",
       "      <th>6304</th>\n",
       "      <th>6305</th>\n",
       "      <th>6306</th>\n",
       "      <th>6307</th>\n",
       "      <th>6308</th>\n",
       "      <th>6309</th>\n",
       "      <th>6310</th>\n",
       "      <th>6311</th>\n",
       "      <th>6312</th>\n",
       "      <th>6313</th>\n",
       "      <th>6314</th>\n",
       "      <th>6315</th>\n",
       "      <th>6316</th>\n",
       "      <th>6317</th>\n",
       "      <th>6318</th>\n",
       "      <th>6319</th>\n",
       "      <th>6320</th>\n",
       "      <th>6321</th>\n",
       "      <th>6322</th>\n",
       "      <th>6323</th>\n",
       "      <th>6324</th>\n",
       "      <th>6325</th>\n",
       "      <th>6326</th>\n",
       "      <th>6327</th>\n",
       "      <th>6328</th>\n",
       "      <th>6329</th>\n",
       "      <th>6330</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>5.307692</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>5.428571</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.3400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>133</td>\n",
       "      <td>6.045455</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>8.125000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.420625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6355 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  '#' in tweet?  '.' in tweet?  ',' in tweet?  ':' in tweet?  \\\n",
       "0  1       True           False          False          False           \n",
       "1  1       False          True           False          False           \n",
       "2  1       False          True           False          False           \n",
       "3  1       True           False          True           False           \n",
       "4  1       True           False          False          False           \n",
       "\n",
       "   ''' in tweet?  'http' in tweet?  number of tokens  number of sentences  \\\n",
       "0  False          False             14                2                     \n",
       "1  False          False             8                 2                     \n",
       "2  True           False             25                2                     \n",
       "3  False          False             9                 1                     \n",
       "4  False          False             18                1                     \n",
       "\n",
       "   number of words  number of nouns  number of verbs  number of adjectives  \\\n",
       "0  13               4                3                0                      \n",
       "1  7                2                0                0                      \n",
       "2  22               7                4                1                      \n",
       "3  8                5                1                0                      \n",
       "4  16               4                3                0                      \n",
       "\n",
       "   number of adverbs  number of pronouns  number of punctuation  tweet length  \\\n",
       "0  0                  1                   0                      69             \n",
       "1  0                  0                   1                      38             \n",
       "2  0                  0                   3                      133            \n",
       "3  0                  0                   0                      65             \n",
       "4  1                  0                   0                      88             \n",
       "\n",
       "   word length (including whitespace)  word per sentence  total hastags  \\\n",
       "0  5.307692                            6.5                1               \n",
       "1  5.428571                            3.5                0               \n",
       "2  6.045455                            11.0               0               \n",
       "3  8.125000                            8.0                1               \n",
       "4  5.500000                            16.0               2               \n",
       "\n",
       "     neg    neu    pos  compound    0         1    2    3    4    5    6    7  \\\n",
       "0  0.000  0.851  0.149  0.2732    0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.286  0.714  0.000 -0.3400    0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.095  0.905  0.000 -0.2960    0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.000  1.000  0.000  0.0000    0.0  0.420625  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.000  1.000  0.000  0.0000    0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "     8    9   10   11   12   13   14   15   16   17   18   19   20   21   22  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    23   24   25  ...  6281  6282  6283  6284  6285  6286  6287  6288  6289  \\\n",
       "0  0.0  0.0  0.0  ...  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    \n",
       "1  0.0  0.0  0.0  ...  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    \n",
       "2  0.0  0.0  0.0  ...  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    \n",
       "3  0.0  0.0  0.0  ...  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    \n",
       "4  0.0  0.0  0.0  ...  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    \n",
       "\n",
       "   6290  6291  6292  6293  6294  6295  6296  6297  6298  6299  6300  6301  \\\n",
       "0  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    \n",
       "1  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    \n",
       "2  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    \n",
       "3  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    \n",
       "4  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    \n",
       "\n",
       "   6302  6303  6304  6305  6306  6307  6308  6309  6310  6311  6312  6313  \\\n",
       "0  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    \n",
       "1  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    \n",
       "2  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    \n",
       "3  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    \n",
       "4  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    \n",
       "\n",
       "   6314  6315  6316  6317  6318  6319  6320  6321  6322  6323  6324  6325  \\\n",
       "0  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    \n",
       "1  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    \n",
       "2  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    \n",
       "3  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    \n",
       "4  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    \n",
       "\n",
       "   6326  6327  6328  6329  6330  \n",
       "0  0.0   0.0   0.0   0.0   0.0   \n",
       "1  0.0   0.0   0.0   0.0   0.0   \n",
       "2  0.0   0.0   0.0   0.0   0.0   \n",
       "3  0.0   0.0   0.0   0.0   0.0   \n",
       "4  0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "[5 rows x 6355 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_dataframe_for_machine_learning(dataframe=train_df, vectorization=False):\n",
    "    \"\"\"Prepares a dataframe for machine learning.\n",
    "    Based on constants specified in the cell above, this function may add columns to the dataframe based on:\n",
    "    keyword search, numeric calculations, sentiment analysis, one hot encoding the keywords,\n",
    "    and/or vectorizing the words.\n",
    "    It wil then drop columns not needed for machine learning.\n",
    "    \n",
    "    Note: Need to pass in a boolean vectorization variable because we can't set up X_test by just calling this\n",
    "    function as vectors will differ with those of X_train (as different tweets have different words).\n",
    "    Vectorization will be kept as part of this function for EDA purposes. However, when preparing submission,\n",
    "    vectorization must be performed separately (and we'd need to pass in vectorization=False)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare a copy, so that experimenting is easy\n",
    "    df = dataframe.copy()\n",
    "    \n",
    "    # Add substring columns\n",
    "    for substring in substring_list:\n",
    "        add_substring_in_tweet_column(substring, df)\n",
    "        \n",
    "    # Add spacy and numeric columns, if true\n",
    "    # Note: have to write df = f(df) for this one as it won't alter the columns like the other functions\n",
    "    if spacy_cols:\n",
    "        df = add_spacy_and_other_numeric_columns(df)\n",
    "    \n",
    "    # Add sentiment analysis\n",
    "    add_sentiment_analysis_column(df, method=sentiment_analysis_method)\n",
    "    \n",
    "    # Encode keyword, if true\n",
    "    if encode_keyword:\n",
    "        encoded_keys = pd.get_dummies(df['keyword'])\n",
    "        df = pd.concat([df, encoded_keys], axis=1)\n",
    "        \n",
    "    # Vectorize tweets, if true\n",
    "    if vectorization:\n",
    "        df = add_vectorization_columns(df=df, method=vectorize_method, min_df=min_df, max_df=max_df, stop_words=stop_words)\n",
    "    \n",
    "    \n",
    "    # Drop columns not wanted for machine learning\n",
    "    df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    # Return prepared dataframe\n",
    "    return df\n",
    "\n",
    "prepared_df = prepare_dataframe_for_machine_learning(train_df, vectorization=True)\n",
    "display(prepared_df.head())\n",
    "#display(prepared_df.corr()['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Scikit Learn Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ridge\n",
      "F1 Scores:\n",
      "[0.64648538 0.5967118  0.68393782]\n",
      "\n",
      "Accuracy Scores:\n",
      "[0.72863332 0.67126527 0.73551439]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_and_prepare_file_skl(prepared_dataframe=prepared_df, ml_model=ml_model, submission=False, \n",
    "                                        train_df=train_df, test_df=test_df):\n",
    "    \"\"\"Takes in a prepared dataframe and a scikit learn machine learning model, and returns the F1 and accuracy \n",
    "    cross-validation scores for when that model is fit to the prepared dataframe. \n",
    "    (Uses a prepared dataframe to save time)\n",
    "    \n",
    "    If submission is set to true, it will fit the model and prepare the file to submit in the format \n",
    "    required by the contest (scores will not be returned). This will use the original, not prepared\n",
    "    dataframes train_df and test_df.\n",
    "    \n",
    "    If no correct model is specified, no scores will be returned.\n",
    "    \n",
    "    Possible ml_models: ridge (Ridge Classifier), knn (K Nearest Neighbors), dt (Decision Tree), xgb (XGBoost)\n",
    "    nb (Naive Bayes - all values must be non-negative), lr (Logistic Regression), svm (Support-Vector Machine),\n",
    "    nn (Neural Network - scikit learn's version)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up machine learning model based on ml_model\n",
    "    \n",
    "    if ml_model == 'ridge':\n",
    "        clf = RidgeClassifier()\n",
    "    \n",
    "    if ml_model == 'knn':\n",
    "        clf = KNeighborsClassifier(n_neighbors=nearest_neighbors)\n",
    "        \n",
    "    if ml_model == 'dt':\n",
    "        clf = DecisionTreeClassifier()\n",
    "        \n",
    "    if ml_model == 'nb':\n",
    "        clf = MultinomialNB()\n",
    "    \n",
    "    if ml_model == 'xgb':\n",
    "#         params = {'objective': 'binary:logistic', 'max_depth': 2, 'learning_rate': 1.0, \n",
    "#                   'silent': True, 'n_estimators': 5}\n",
    "#         clf = XGBClassifier(**params)\n",
    "        clf = XGBClassifier()\n",
    "    \n",
    "    if ml_model == 'lr':\n",
    "        clf = LogisticRegression(solver='liblinear',max_iter=1000)\n",
    "        \n",
    "    if ml_model == 'svm':\n",
    "        clf = SVC(gamma='auto')\n",
    "    \n",
    "    if ml_model == 'nn':\n",
    "        clf = MLPClassifier(solver='lbfgs', alpha=1e-5, \n",
    "                            hidden_layer_sizes=(nn_num_neurons, nn_hidden_layers), \n",
    "                            random_state=1)\n",
    "    \n",
    "    \n",
    "    try:    \n",
    "        \n",
    "        if submission:\n",
    "            # Prepare file for submission\n",
    "\n",
    "            # Make a copy of the dataframes\n",
    "            train = train_df.copy()\n",
    "            test = test_df.copy()\n",
    "\n",
    "\n",
    "            # Vectorize, if desired, first so that we can properly fit test\n",
    "            if vectorize_method:\n",
    "                # Set up vectorizer\n",
    "                if vectorize_method == 'count':\n",
    "                    vectorizer = CountVectorizer(min_df=min_df, max_df=max_df, stop_words=stop_words)\n",
    "                if vectorize_method == 'tfidf':\n",
    "                    vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, stop_words=stop_words)  \n",
    "\n",
    "                # Create vectors and add to each dataframe\n",
    "                train_vectors = pd.DataFrame(vectorizer.fit_transform(train[\"text\"]).todense())\n",
    "                test_vectors = pd.DataFrame(vectorizer.transform(test[\"text\"]).todense())\n",
    "                train = pd.concat([train, train_vectors], axis=1)\n",
    "                test = pd.concat([test, test_vectors], axis=1)\n",
    "\n",
    "\n",
    "            # Prepare the dataframes for machine learning\n",
    "            prepared_train = prepare_dataframe_for_machine_learning(train, vectorization=False)\n",
    "            prepared_test = prepare_dataframe_for_machine_learning(test, vectorization=False)\n",
    "\n",
    "\n",
    "            # Perform machine learning and save the submission file\n",
    "            clf.fit(prepared_train.drop(\"target\", axis=1), prepared_train[\"target\"])\n",
    "            sample_submission = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "            sample_submission[\"target\"] = clf.predict(prepared_test)\n",
    "            sample_submission.to_csv(\"../submission.csv\", index=False)\n",
    "\n",
    "\n",
    "        else: \n",
    "            # Return f1 scores\n",
    "            df = prepared_dataframe.copy()\n",
    "            f1_scores = cross_val_score(clf, df.drop(\"target\", axis=1),\n",
    "                                                        df[\"target\"], cv=3, scoring=\"f1\")\n",
    "            accuracy_scores = cross_val_score(clf, df.drop(\"target\", axis=1),\n",
    "                                                              df[\"target\"], cv=3, scoring=\"accuracy\")\n",
    "            print('F1 Scores:')\n",
    "            print(f1_scores)\n",
    "            print()\n",
    "            print('Accuracy Scores:')\n",
    "            print(accuracy_scores)\n",
    "    \n",
    "    \n",
    "    except:\n",
    "        print('No valid model entered/incorrect vectorization method!')\n",
    "        \n",
    "\n",
    "model_list = ['ridge', 'knn', 'dt', 'xgb', 'lr', 'svm', 'nn']\n",
    "model_list = ['ridge']\n",
    "for model in model_list:\n",
    "    print('Model:', model)\n",
    "    evaluate_model_and_prepare_file_skl(prepared_dataframe=prepared_df, ml_model=model, submission=False)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model_and_prepare_file_skl(train_df=train_df, test_df=test_df, ml_model=ml_model, submission=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Model performs best with vectorization. ML model that performs best with vectorization is the ridge classifier and logistic regression. I haven't formally tested it, but they seem efficient too, so I'll just look at those for now until I add neural nets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update: Experimented with neural nets. Scikit learn's version does not perform as well as some of the other models. However, Keras' does. Since the above function is specific to scikit learn models, I'll create a new function for Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 12710)             80772050  \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 12710)             161556810 \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 2)                 25422     \n",
      "=================================================================\n",
      "Total params: 242,354,282\n",
      "Trainable params: 242,354,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_and_prepare_file_keras(train_df=train_df, test_df=test_df, \n",
    "                                          epochs=10, neurons=nn_num_neurons, hidden_layers=nn_hidden_layers):\n",
    "    \"\"\"Similar to evaluate_model_and_prepare_file_skl except this one will only train a Keras neural network.\n",
    "    Unlike the previous one, this function will take in the dimensions of the neural network and the number of\n",
    "    epochs as this function can be more specific.\n",
    "    \n",
    "    As training a neural network is very slow, unlike the previous function, this one will both print out\n",
    "    the confusion matrix and classification report, and prepare the file for submission. To do this,\n",
    "    it will prepare the dataframes for machine learning and won't take in a prepared dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # First, prepare the dataframes for machine learning\n",
    "    \n",
    "    # Make a copy of the dataframes\n",
    "    train = train_df.copy()\n",
    "    test = test_df.copy()\n",
    "    \n",
    "    # Vectorize, if desired, first so that we can properly fit test\n",
    "    if vectorize_method:\n",
    "        # Set up vectorizer\n",
    "        if vectorize_method == 'count':\n",
    "            vectorizer = CountVectorizer(min_df=min_df, max_df=max_df, stop_words=stop_words)\n",
    "        if vectorize_method == 'tfidf':\n",
    "            vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, stop_words=stop_words)  \n",
    "\n",
    "        # Create vectors and add to each dataframe\n",
    "        train_vectors = pd.DataFrame(vectorizer.fit_transform(train[\"text\"]).todense())\n",
    "        test_vectors = pd.DataFrame(vectorizer.transform(test[\"text\"]).todense())\n",
    "        train = pd.concat([train, train_vectors], axis=1)\n",
    "        test = pd.concat([test, test_vectors], axis=1)\n",
    "\n",
    "    # Use the function to finish preparing the dataframes\n",
    "    prepared_train = prepare_dataframe_for_machine_learning(train, vectorization=False)\n",
    "    prepared_test = prepare_dataframe_for_machine_learning(test, vectorization=False)\n",
    "\n",
    "    \n",
    "    # Next, set up the neural network\n",
    "    \n",
    "    # Separate X and y, and make y categorical\n",
    "    X = prepared_train.drop(\"target\", axis=1)\n",
    "    y = to_categorical(prepared_train[\"target\"])\n",
    "\n",
    "    # We also need to scale the data to help the performance of the model\n",
    "    scaler_object = MinMaxScaler()\n",
    "    scaler_object.fit(X)\n",
    "    X = scaler_object.transform(X)\n",
    "    X_submission = scaler_object.transform(prepared_test)\n",
    "    \n",
    "    # Set up the neural network. These features can be adjusted later.    \n",
    "    model = Sequential()\n",
    "    for i in range(0, hidden_layers):\n",
    "        model.add(Dense(neurons, input_dim=X.shape[1], activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    \n",
    "    # Perform machine learning and save the submission file\n",
    "    model.fit(X, y, epochs=epochs, verbose=2)\n",
    "    sample_submission = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "    sample_submission[\"target\"] = model.predict_classes(X_submission)\n",
    "    sample_submission.to_csv(\"../submission.csv\", index=False)\n",
    "    \n",
    "    # May add these later, but obviously need to do train test split\n",
    "    # Right now using my submissions for testing so I don't need it\n",
    "    # print(confusion_matrix(y_test.argmax(axis=1),predictions))\n",
    "    # print(classification_report(y_test.argmax(axis=1),predictions))\n",
    "\n",
    "\n",
    "evaluate_model_and_prepare_file_keras(train_df=train_df, test_df=test_df, \n",
    "                                      epochs=10, neurons=2*prepared_df.shape[1], hidden_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = prepared_df.copy()\n",
    "\n",
    "# X = df.drop(\"target\", axis=1)\n",
    "# y = to_categorical(df[\"target\"])\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# scaler_object = MinMaxScaler()\n",
    "# scaler_object.fit(X_train)\n",
    "# scaled_X_train = scaler_object.transform(X_train)\n",
    "# scaled_X_test = scaler_object.transform(X_test)\n",
    "\n",
    "# dim = X.shape[1]\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(2*dim, input_dim=dim, activation='relu'))\n",
    "# model.add(Dense(2*dim, input_dim=dim, activation='relu'))\n",
    "# model.add(Dense(2, activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# model.fit(scaled_X_train,y_train,epochs=10, verbose=2)\n",
    "\n",
    "# predictions = model.predict_classes(scaled_X_test)\n",
    "\n",
    "# # print(confusion_matrix(y_test.argmax(axis=1),predictions))\n",
    "# # print(classification_report(y_test.argmax(axis=1),predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
